<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Failed to create voting files on disk group RECOC1 | My New Hugo Site</title>
    <link rel="stylesheet" href="/quickstart2/css/style.css" />
    <link rel="stylesheet" href="/quickstart2/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/quickstart2/">Home</a></li>
      
      <li><a href="/quickstart2/blog1/">Blog 1</a></li>
      
      <li><a href="/quickstart2/blog2/">Blog 2</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Failed to create voting files on disk group RECOC1</span></h1>
<h2 class="author">Sidhu</h2>
<h2 class="date">2017/04/28</h2>
</div>

<main>
<p>Long story short, faced this issue while running OneCommand for one Exadata system. The root.sh step (Initialize Cluster Software) was failing with the following error on the screen</p>
<p><!-- raw HTML omitted -->Checking file root_dm01dbadm02.in.oracle.com_2017-04-27_18-13-27.log on node dm01dbadm02.somedomain.com<br>
Error: Error running root scripts, please investigate…<br>
Collecting diagnostics…<br>
Errors occurred. Send /u01/onecommand/linux-x64/WorkDir/Diag-170427_181710.zip to Oracle to receive assistance.<!-- raw HTML omitted --></p>
<p>Doesn’t make much sense. So let us check the log file of this step</p>
<p><!-- raw HTML omitted -->2017-04-27 18:17:10,463 [INFO][ OCMDThread][ ClusterUtils:413] Checking file root_dm01dbadm02.somedomain.com_2017-04-27_18-13-27.log on node inx321dbadm02.somedomain.com<br>
2017-04-27 18:17:10,464 [INFO][ OCMDThread][ OcmdException:62] Error: Error running root scripts, please investigate…<br>
2017-04-27 18:17:10,464 [FINE][ OCMDThread][ OcmdException:63] Throwing OcmdException… message:Error running root scripts, please investigate…<!-- raw HTML omitted --></p>
<p>So we need to go to root.sh log file now. That shows</p>
<p><!-- raw HTML omitted -->Failed to create voting files on disk group RECOC1.<br>
Change to configuration failed, but was successfully rolled back.<br>
CRS-4000: Command Replace failed, or completed with errors.<br>
Voting file add failed<br>
2017/04/27 18:16:37 CLSRSC-261: Failed to add voting disks<!-- raw HTML omitted --></p>
<p><!-- raw HTML omitted -->Died at /u01/app/12.1.0.2/grid/crs/install/crsinstall.pm line 2068.<br>
The command ‘/u01/app/12.1.0.2/grid/perl/bin/perl -I/u01/app/12.1.0.2/grid/perl/lib -I/u01/app/12.1.0.2/grid/crs/install /u01/app/12.1.0.2/grid/crs/install/root<br>
crs.pl ‘ execution failed<!-- raw HTML omitted --></p>
<p>Makes some senses but we can’t understand what happened while creating Voting files on RECOC1. Let us check ASM alert log also</p>
<p><!-- raw HTML omitted -->NOTE: Creating voting files in diskgroup RECOC1<br>
Thu Apr 27 18:16:36 2017<br>
NOTE: Voting File refresh pending for group 1/0x39368071 (RECOC1)<br>
Thu Apr 27 18:16:36 2017<br>
NOTE: Attempting voting file creation in diskgroup RECOC1<br>
NOTE: voting file allocation (replicated) on grp 1 disk RECOC1_CD_00_DM01CELADM01<br>
NOTE: voting file allocation on grp 1 disk RECOC1_CD_00_DM01CELADM01<br>
NOTE: voting file allocation (replicated) on grp 1 disk RECOC1_CD_00_DM01CELADM02<br>
NOTE: voting file allocation on grp 1 disk RECOC1_CD_00_DM01CELADM02<br>
NOTE: voting file allocation (replicated) on grp 1 disk RECOC1_CD_00_DM01CELADM03<br>
NOTE: voting file allocation on grp 1 disk RECOC1_CD_00_DM01CELADM03<br>
ERROR: Voting file allocation failed for group RECOC1<br>
Thu Apr 27 18:16:36 2017<br>
Errors in file /u01/app/oracle/diag/asm/+asm/+ASM1/trace/+ASM1_ora_228588.trc:<br>
<strong><!-- raw HTML omitted -->ORA-15274: Not enough failgroups (5) to create voting files<!-- raw HTML omitted --></strong><!-- raw HTML omitted --></p>
<p>So we can see the issue here. We can look at the above trace file also for more detail.</p>
<p>Now to why did this happen ?</p>
<p>The RECOC1 is a HIGH redundancy disk group which means that if we want to place Voting files there, it should have at least 5 failure groups. In this configuration there are only 3 cells and that doesn’t meet the minimum failure groups condition (1 cell = 1 failgroup in Exadata).</p>
<p>Now to how did it happen ?</p>
<p>This one was an Exadata X3 half rack and we planned to deploy it (for testing purpose) as 2 quarter racks : 1st cluster with db1, db2 + cell1, cell2, cell3 and 2nd cluster with db3, db4 + cell4, cell5, cell6, cell7. All the disk groups to be in High redundancy.</p>
<p>Before a certain 12.x Exadata software version it was not even possible to have all disk groups in High redundancy in a quarter rack as to have Voting disk in a High redundancy disk group you need to have a minimum of 5 failure groups (as mentioned above). In a quarter rack you have only 3 fail groups. With a certain 12.x Exadata software version a new feature quorum disks was introduced which made is possible to have that configuration. Read <a href="http://asmsupportguy.blogspot.in/2016/03/quorum-disks-in-exadata.html">this link</a> for more details. Basically we take a slice of disk from each DB node and add it to the disk group where you want to have the Voting file. 3 cells + 2 disks from DB nodes makes it 5 so all is good.</p>
<p>Now while starting with the deployment we noticed that db node1 was having some hardware issues. As we needed the machine for testing so we decided to build the first cluster with 1 db node only. So the final configuration of 1st cluster had 1 db node + 3 cells. We imported the XML back in OEDA, modified the cluster 1 configuration to 1 db node and generated the configuration files. That is where the problem started. The RECO disk group still was High redundancy but as we had only 1 db node at this stage so the configuration was not even a candidate for quorum disks. Hence the above error. Changing DBFS_DG to Normal redundancy fixed the issue as when DBFS_DG is Normal redundancy, OneCommand will place the Voting files there.</p>
<p>Ideally it shouldn’t happened as OEDA shouldn’t allow a configuration that is not doable. The case here is that as originally the configuration was having 2 db nodes + 3 cells so High redundancy for all disk groups was allowed in OEDA. While modifying the configuration when one db node was removed from the cluster, OEDA probably didn’t run the redundancy check on disk groups and it allowed the go past that screen. If you try to create a new configuration with 1 db node + 3 cells, it will not allow you to choose High redundancy for all disk groups. DBFS will remain in Normal redundancy. You can’t change that.</p>

</main>

  <footer>
  
  
  </footer>
  </body>
</html>

